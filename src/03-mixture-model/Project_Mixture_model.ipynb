{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2yYLbrU-air"
   },
   "source": [
    "### Names of Students and ID: Vincent Eichhorn, Josef Pribbernow\n",
    "### Group number: J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Hie1dDo-ais"
   },
   "source": [
    "# Clustering of image data with Mixture Models\n",
    "\n",
    "\n",
    "We saw in the last project that mixture models / k-means are a powerful way to detect groups in multidimensional real data. However, mixture models can be easily generalized to other types of data.\n",
    "The goal of the project is to implement a mixture model that can cluster image data using a mixture of bernoulli distributions. \n",
    "This jupyter notebook will serve as a guide for the different steps of the implementation of the model and of its estimation. In the last part we will try to apply our model to recover missing part of an image! This is also called imputation.\n",
    "\n",
    "You will find expected outputs throughout the notebook that should match your output for the same example input, given the correct implementation of the function bodys.\n",
    "\n",
    "`Note:` Throughout the notebook you will find docstrings describing the in and output of the different functions, these are meant as hints to guide during the implementation. Whether you meet the exact type or not is up to you and will not be graded as long as your implementation returns the expected result. If your implementation is slow it might be sufficient to use half of the dataset for good results.\n",
    "\n",
    "### For grading, you are just expected to fill in this notebook. Feel free to add text answers within the notebook whenever appropriate.\n",
    "\n",
    "#### Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some helpful packages to solve this exercise\n",
    "# You might need to restart the notebook kernel for these to be available for import after the installation\n",
    "# The tensorflow package is only needed to load the MNIST datasets. You can avoid to installing it if you \n",
    "# plan to download the table on your own.\n",
    "%pip install tensorflow\n",
    "\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install scipy\n",
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbDHSArABaCA"
   },
   "source": [
    "# Know your data\n",
    "Have a look at the `MNIST` data set below; Get familiar with how the data is structured, this will be important for later excercises as well!\n",
    "\n",
    "We will use prepared versions of the datasets that are already available in [Keras](https://keras.io/getting_started/). You will need to install `keras` with `Tensorflow`. Alternatively there are also tabular versions to download on Kaggle of the [digit](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv) and the [fashion](https://www.kaggle.com/datasets/zalando-research/fashionmnist) datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnGtKV9kBZhW",
    "outputId": "d9271c45-31dc-492d-8359-51862662271b"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libffi.so.7: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _tf_keras \u001b[38;5;28;01mas\u001b[39;00m _tf_keras\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/_tf_keras/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/_tf_keras/keras/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callbacks \u001b[38;5;28;01mas\u001b[39;00m callbacks\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/activations/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deserialize \u001b[38;5;28;01mas\u001b[39;00m deserialize\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get \u001b[38;5;28;01mas\u001b[39;00m get\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialize \u001b[38;5;28;01mas\u001b[39;00m serialize\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/activations/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m celu\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exponential\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/activations/activations.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/backend/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# When using the torch backend,\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# torch needs to be imported first, otherwise it will segfault\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# upon import.\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/backend/config.py:448\u001b[39m\n\u001b[32m    445\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    446\u001b[39m         _NNX_ENABLED = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[43mset_nnx_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_NNX_ENABLED\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/backend/config.py:249\u001b[39m, in \u001b[36mset_nnx_enabled\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_nnx_enabled\u001b[39m(value):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mglobal\u001b[39;00m _NNX_ENABLED\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m global_state\n\u001b[32m    251\u001b[39m     _NNX_ENABLED = \u001b[38;5;28mbool\u001b[39m(value)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _NNX_ENABLED:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/backend/common/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_utils\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutocastScope\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Variable \u001b[38;5;28;01mas\u001b[39;00m KerasVariable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/backend/common/dtypes.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m standardize_dtype\n\u001b[32m      7\u001b[39m BOOL_TYPES = (\u001b[33m\"\u001b[39m\u001b[33mbool\u001b[39m\u001b[33m\"\u001b[39m,)\n\u001b[32m      8\u001b[39m INT_TYPES = (\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muint8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muint16\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mint64\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/backend/common/variables.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstateless_scope\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_stateless_scope\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstateless_scope\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m in_stateless_scope\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mVariable\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/utils/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio_dataset_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio_dataset_from_directory\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/utils/audio_dataset_utils.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset_utils\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow_io \u001b[38;5;28;01mas\u001b[39;00m tfio\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/utils/dataset_utils.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m file_utils\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/tree/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assert_same_paths\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assert_same_structure\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flatten\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/tree/tree_api.py:13\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m torchtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m optree.available:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dmtree.available:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dmtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/keras/src/tree/optree_impl.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_structures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_structures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _DictWrapper\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/tensorflow/__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/cs-apml-assignments-mElxrNIC-py3.12/lib/python3.12/site-packages/tensorflow/python/pywrap_tensorflow.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"A Python wrapper that loads _pywrap_tensorflow_internal.so.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# importing hashlib to avoid a linkage issue. refer b/372709714\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhashlib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.6/lib/python3.12/ctypes/__init__.py:8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_types\u001b[39;00m\n\u001b[32m      6\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m1.1.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_ctypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Union, Structure, Array\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_ctypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _Pointer\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_ctypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CFuncPtr \u001b[38;5;28;01mas\u001b[39;00m _CFuncPtr\n",
      "\u001b[31mImportError\u001b[39m: libffi.so.7: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(X_train, Y_train), (test_X, test_y) = mnist.load_data()\n",
    "X_train = np.concatenate([X_train, test_X], axis=0)\n",
    "Y_train = np.concatenate([Y_train, test_y], axis=0)\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(Y_train.shape))\n",
    "\n",
    "example_1_X = X_train[8]\n",
    "example_3_X = X_train[7]\n",
    "example_7_X = X_train[40000]\n",
    "example_7_y = Y_train[40000]\n",
    "example_8_X = X_train[17]\n",
    "print(f\"This is a 7: \\n{example_7_y}\")\n",
    "print(f\"And it has shape: \\n{example_7_X.shape}\")\n",
    "print(f\"This is a 7's image data: \\n{example_7_X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luIBVKCF-ais"
   },
   "source": [
    "### 1) Get familiar with the dataset by plotting some of the $x_i$ in $X_{\\text{train}}$.\n",
    "\n",
    "From the last section you might have seen that the value matrices $x \\in \\mathbb{N}^{d\\times d} $ are actually 2D numpy arrays of shape (d, d). These resemble the pixels in the image, where each value indicates the respective pixels intensity.\n",
    "\n",
    "Have a look at how this translates into the actual image by plotting some examples below, this will come in handy later on!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "eJ7E8SXeYOv4",
    "outputId": "fe3f29d2-0700-42e6-cf81-c33d350af57f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_7_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Plot some examples from X_train using matplotlib\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mexample_7_X\u001b[49m, cmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrey\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example_7_X' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot some examples from X_train using matplotlib\n",
    "plt.imshow(example_7_X, cmap = 'grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCuFy9-Gswx2"
   },
   "source": [
    "## 2) Preprocessing\n",
    "\n",
    "You can see in the plot that the value matrices of the different $x$ contain values $\\in \\{0, 1, ..., 255\\}$, representing pixel intensity on the grey scale. However, we want to reduce dimensionality for this project and transform all values to be in ${0,1}$ while keeping a meaningful representation of the original image. This will help us later when we want to think more in terms of probabilities, e.g., the probability for a pixel being on (white) given its image class. We can then model each pixel as an independant Bernoulli for our approximation. In addition, we will later work on (flattened matrices) vectors $\\vec{v} \\in \\mathbb{N}^{d*d}$ instead of matrices $x \\in \\mathbb{N}^{d\\times d}$.\n",
    "\n",
    "2.a) Implement the fuction body of `binarize_image()` that flattens a given input matrix of dimension $d \\times d$ and returns a binarized vector of dimension $d^2$, given a binarization `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YH-IF4KL-ais",
    "outputId": "6e8b54a5-5402-4599-9db3-6105f7dc052d"
   },
   "outputs": [],
   "source": [
    "def binarize_image(xi_mat: np.ndarray, threshold: int = 127):\n",
    "    \"\"\"\n",
    "    Convert 2D value matrix of shape (d, d) of greyscale image to 1D binary vector of shape (d * d).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xi_mat : np.ndarray\n",
    "        A 2D NumPy array of shape (d, d) representing a grayscale image,\n",
    "        where each element is a pixel intensity value between 0 and 255.\n",
    "\n",
    "    threshold: int\n",
    "        Decision threshold value above which all pixel values are set to 1,\n",
    "        below (or equal to) which they are set to 0. Default is 127.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xi_bin_vec : np.array\n",
    "        A 1D NumPy array of shape (d * d) representing the binary vector\n",
    "        of the input image, where each element is either 0 or 1 (binary vector).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten the 2D image matrix to a 1D vector\n",
    "    xi_flat = xi_mat.flatten()\n",
    "\n",
    "    # Binarize the flattened vector based on the threshold\n",
    "    xi_bin_vec = np.where(xi_flat > threshold, 1, 0)\n",
    "\n",
    "    return xi_bin_vec\n",
    "  \n",
    "\n",
    "test_mat = np.array([\n",
    "    [0, 127, 255],\n",
    "    [255, 128, 0],\n",
    "    [0, 126, 255]\n",
    "])\n",
    "\n",
    "test_mat = np.array([\n",
    "       [ 80,  80,   0,   0,  80,   0,   0,  80,  80,  80],\n",
    "       [ 80,   0, 255, 255,   0, 255, 255,   0,  80,  80],\n",
    "       [ 80,   0, 255, 255, 255,   0, 255,   0,  80,  80],\n",
    "       [ 80,  80,   0,   0,   0,   0,   0,   0,  80,  80],\n",
    "       [ 80,   0,   0, 192,   0, 192, 192, 192,   0,  80],\n",
    "       [  0, 192,   0, 192,   0, 192, 192, 192, 192,   0],\n",
    "       [  0, 192,   0, 192,   0, 192, 192,  50, 192,   0],\n",
    "       [  0, 192,   0, 192,   0, 192, 192, 192, 192,   0],\n",
    "       [ 80,   0,   0, 192,   0, 192, 192, 192,   0,  80],\n",
    "       [ 80,  80,   0,   0,   0,   0,   0,   0,  80,  80]\n",
    "])\n",
    "\n",
    "test_bin_vec = np.array([0, 0, 1, 1, 1, 0, 0, 0, 1])\n",
    "\n",
    "test_bin_vec = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
    "                         1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "                         0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
    "                         1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
    "                         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Check if your function returns the expected result by comparing the matrices below\n",
    "print(f\"Expected binarized vector:\\n{test_bin_vec}\")\n",
    "print(f\"Binarized vector with your implementation:\\n{binarize_image(test_mat)}\")\n",
    "\n",
    "assert np.array_equal(binarize_image(test_mat), test_bin_vec), \"Binarized vector returned by your implementation is not equal to the expected vector.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juVHATprPpFk"
   },
   "source": [
    "Printed matrices are nice, but you can never _bee_ too sure with these things. \n",
    "\n",
    "#### 2.b) Implement the helper function `recover_image_matrix()` to recover a binarized matrix from the vector. Plot the binarized representation matrix and have a look at how the figure changed!\n",
    "\n",
    "After running the code on the `test_mat`, feel free to look at some other examples in the `MNIST` dataset. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FzG9hpmX-CL",
    "outputId": "ba42a179-9307-455a-e0f8-550122be1668"
   },
   "outputs": [],
   "source": [
    "def recover_image_matrix(xi_bin_vec: np.ndarray, d: int = 28):\n",
    "    \"\"\"\n",
    "    Convert 1D binary vector of shape (d * d) to 2D binary matrix of shape (d, d) of the image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xi_bin_vec : np.array\n",
    "        A 1D NumPy array of shape (d * d) representing the binary representation\n",
    "        of the input image, where each element is either 0 or 1.\n",
    "\n",
    "    d : int\n",
    "        The dimension of the square matrix to be recovered.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xi_bin_mat : np.ndarray\n",
    "        A 2D NumPy array of shape (d, d) representing the grayscale image,\n",
    "        where each element is either 0 or 1 (binary matrix).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape the 1D binary vector to a 2D binary matrix of shape (d, d)\n",
    "    xi_bin_mat = xi_bin_vec.reshape((d, d))\n",
    "    \n",
    "    return xi_bin_mat\n",
    "\n",
    "\n",
    "test_bin_mat = np.array([\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "test_bin_mat = np.array([\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 1, 0, 1, 1, 0, 0, 0],\n",
    "       [0, 0, 1, 1, 1, 0, 1, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 1, 0, 1, 1, 1, 0, 0],\n",
    "       [0, 1, 0, 1, 0, 1, 1, 1, 1, 0],\n",
    "       [0, 1, 0, 1, 0, 1, 1, 0, 1, 0],\n",
    "       [0, 1, 0, 1, 0, 1, 1, 1, 1, 0],\n",
    "       [0, 0, 0, 1, 0, 1, 1, 1, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "print(f\"Expected binarized matrix:\\n{test_bin_mat}\")\n",
    "print(f\"Binarized matrix with your implementation:\\n{recover_image_matrix(test_bin_vec, 10)}\")\n",
    "\n",
    "assert np.array_equal(recover_image_matrix(test_bin_vec, 10), test_bin_mat), \"Binarized matrix returned by your implementation is not equal to the expected matrix.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "hfr5n6HBPtde",
    "outputId": "4522b218-193d-451b-9678-c7cb2274f655"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_mat, cmap = 'grey')\n",
    "plt.title(\"Grey scale value matrix\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_bin_mat, cmap = 'grey')\n",
    "plt.title(\"Binarized value matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What do we observe?*\n",
    "\n",
    "The recovered image only consists out of totally black or totally white pixels. That's caused by the binariziation mapping pixel values below the threshold to white and above the threshold to black."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5hBaKH4BOR1"
   },
   "source": [
    "## 3) Class representatives\n",
    "\n",
    "Let's get back to the `MNIST` dataset. This dataset was prepared for a supervised learning task. For each $x \\in X_{\\text{train}}$ we have a label $y \\in Y_{\\text{train}}$. We can partition $X_{\\text{train}}$ into $|C|$ subsets that correspond to the images for each of the digits ($X^{(1)}$ are all the samples for digit $1$, $X^{(2)}$ for digit $2$, and so on).\n",
    "\n",
    "$X^{(c)} = \\{x \\in X_{\\text{train}} \\mid y = c\\}, \\text{ with } c \\in C = \\{0,...9\\}$\n",
    "\n",
    "### 3.a) Compute a representative vector for a given class.\n",
    "Apply the binary matrix transformations from earlier sections to the training dataset and implement the function body for `compute_class_representative()` that computes a representative value vector (flattened matrix) for a set of input matrices. Then use this function to compute representatives for $c = \\{1,3,8\\}$.\n",
    "In this case you can think of the representative value matrix (or vector) as a matrix (or vector) in which each value resembles the probability of the corresponding pixel being white (= a value of 1). This probability is computed given all matrices (or vectors) of that class. You will compute a maximum a posteriori estimate. You may use pseudo-counts if neccessary.\n",
    "\n",
    "\n",
    "`Note:` For simplicity we assume that pixels in the image data are independent of each other. Can you think of a reason why we do this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "DFh4PZtfO4th",
    "outputId": "cdbba28e-80da-4c9e-856d-564563fdd60f"
   },
   "outputs": [],
   "source": [
    "def compute_class_representative(X_c: np.ndarray, pseudo_count = 1e-3):\n",
    "    \"\"\"\n",
    "    Compute a representative value matrix for a set of input matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_c : np.ndarray\n",
    "        2D array of binarized input vectors of shape (num_samples, d).\n",
    "    pseudo_count: float \n",
    "        pseudo count to be added to each pixel value.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    class_representative : np.array\n",
    "        1D array (n_features,) representing the class representative vector.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the mean of each column (pixel position) across all samples\n",
    "    mean_vector = np.mean(X_c, axis=0)\n",
    "\n",
    "    # Add pseudo count to each element of the mean vector\n",
    "    class_representative = mean_vector + pseudo_count\n",
    "\n",
    "    return class_representative\n",
    "    \n",
    "\n",
    "\n",
    "test_X_c = np.random.default_rng(161).integers(0, 2, (25, 24))\n",
    "test_representative = np.array([0.441, 0.681, 0.641, 0.601, 0.321, 0.521, 0.401, 0.521, 0.481, 0.561, 0.561, 0.481, 0.481, 0.641, 0.481, 0.321, 0.481, 0.521, 0.481, 0.521, 0.441, 0.601, 0.601, 0.401])\n",
    "\n",
    "print(f\"Expected representative matrix: \\n{test_representative}\")\n",
    "print(f\"Representative matrix with your implementation: \\n{compute_class_representative(test_X_c)}\")\n",
    "\n",
    "assert np.array_equal(compute_class_representative(test_X_c), test_representative), \"Class representative matrix returned by your implementation is not equal to the expected class representative matrix.\"\n",
    "\n",
    "# Once you implemented the function body above, create subsets X_1, X_3, X_8\n",
    "# from X_train and compute their representative vectors and plot them below. Use\n",
    "# appropriate transformation to get back to plotable matrix data with correct\n",
    "# dimensions.\n",
    "\n",
    "# Apply binarization\n",
    "X_train_bin = np.array([binarize_image(x) for x in X_train])\n",
    "\n",
    "# Compute representatives\n",
    "representative_X_1_vec = compute_class_representative(X_train_bin[Y_train == 1])\n",
    "representative_X_3_vec = compute_class_representative(X_train_bin[Y_train == 3])\n",
    "representative_X_8_vec = compute_class_representative(X_train_bin[Y_train == 8])\n",
    "\n",
    "# Make plotable\n",
    "representative_X_1_mat = recover_image_matrix(representative_X_1_vec, 28)\n",
    "representative_X_3_mat = recover_image_matrix(representative_X_3_vec, 28)\n",
    "representative_X_8_mat = recover_image_matrix(representative_X_8_vec, 28)\n",
    "\n",
    "# Binarize examples\n",
    "example_1_X_bin_vec = binarize_image(example_1_X)\n",
    "example_3_X_bin_vec = binarize_image(example_3_X)\n",
    "example_8_X_bin_vec = binarize_image(example_8_X)\n",
    "\n",
    "# Make plotable\n",
    "example_1_X_bin_mat = recover_image_matrix(example_1_X_bin_vec, 28)\n",
    "example_3_X_bin_mat = recover_image_matrix(example_3_X_bin_vec, 28)\n",
    "example_8_X_bin_mat = recover_image_matrix(example_8_X_bin_vec, 28)\n",
    "\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.imshow(representative_X_1_mat, cmap = 'grey')\n",
    "plt.title(\"Representative of $X^{(1)}$\")\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.imshow(representative_X_3_mat, cmap = 'grey')\n",
    "plt.title(\"Representative of $X^{(3)}$\")\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.imshow(representative_X_8_mat, cmap = 'grey')\n",
    "plt.title(\"Representative of $X^{(8)}$\")\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.imshow(example_1_X_bin_mat, cmap = 'grey')\n",
    "plt.title(\"Binary example of $X^{(1)}$\")\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.imshow(example_3_X_bin_mat, cmap = 'grey')\n",
    "plt.title(\"Binary example of $X^{(3)}$\")\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.imshow(example_8_X_bin_mat, cmap = 'grey')\n",
    "plt.title(\"Binary example of $X^{(8)}$\")\n",
    "\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.imshow(example_1_X, cmap = 'grey')\n",
    "plt.title(\"Example of $X^{(1)}$\")\n",
    "plt.subplot(3, 3, 8)\n",
    "plt.imshow(example_3_X, cmap = 'grey')\n",
    "plt.title(\"Example of $X^{(3)}$\")\n",
    "plt.subplot(3, 3, 9)\n",
    "plt.imshow(example_8_X, cmap = 'grey')\n",
    "plt.title(\"Example of $X^{(8)}$\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Can you think of a reason why we do this?*\n",
    "Currently not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LN6CUGLb-ait"
   },
   "source": [
    "## Computing vector probabilities\n",
    "\n",
    "#### 3.b) Write a function that computes the log probability for a set of binarized vectors\n",
    "\n",
    "It will take as input a set of input vectors $X$ you derived from the set of `MNIST` image matrices $X_{\\text{test}}$, a class representative $\\vec{\\mu}_c$. Be careful not to take the logarithm of zero (to avoid that we add a parameter $\\varepsilon$ to the function).\n",
    "\n",
    "Later we can use this to compute for example:\n",
    "\n",
    "$$\n",
    "\\log \\mathbb{P} (\\vec{v} \\mid y = c) = \\sum_{j=1}^{784} v_j\\cdot \\log(\\mu_{c,j}) + (1-v_j)\\cdot \\log (1-\\mu_{c,j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uB7KoTXE-aiu",
    "outputId": "ab27566c-679d-405f-cb97-6ddc9702b0eb"
   },
   "outputs": [],
   "source": [
    "def log_probabilities(X, class_representative, eps = 1e-7):\n",
    "    \"\"\"\n",
    "    Compute log P(x_i | y=c) for binary X under independent Bernoulli pixels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, d)\n",
    "        Binary inputs (0/1). If floats, values are interpreted as probabilities but should be 0/1.\n",
    "    class_representative : array-like, shape (d,)\n",
    "        Per-pixel probability of 1 (values in [0,1]).\n",
    "    eps: float\n",
    "        minimum value for the probability of a pixel to be observed (to avoid rounding errors).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_probas : np.ndarray, shape (n_samples,)\n",
    "        Log-likelihoods for each sample.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    p = np.clip(class_representative, eps, 1.0 - eps)\n",
    "\n",
    "\n",
    "test_log_probas = np.array([-87.87645595, -100.75135407, -74.28714557, -82.9868483, -86.35217585, -164.77564207, -79.17198904, -95.40878212])\n",
    "your_log_probas = log_probabilities(X_train_bin[Y_train == 1][:8], compute_class_representative(X_train_bin[Y_train == 1][:8]))\n",
    "\n",
    "print(f\"Expected log probabilities:\\n{test_log_probas}\")\n",
    "print(f\"Log probabilities with your implementation:\\n{your_log_probas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mpW1l2K-aiu"
   },
   "source": [
    "## Compute the class posteriors\n",
    "\n",
    "### 3.c) Implement `compute_class_posteriors`. \n",
    "\n",
    "This function takes as argument a set of representatives, and the class priors and returns a matrix of all the class posterior for each image vector in the input. You can use the function `log_probabilities()` to simplify the computation. You might also want to consider the log-sum-exp transformation to improve the numerical stability of your calculations where appropriate (there is for instance a `scipy.special.logsumexp()` function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUdEk1YW-aiu",
    "outputId": "934df3f0-68ac-42bb-f28f-ecbde8ea331c"
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "def compute_class_posteriors(X, class_representative_lst, priors_lst):\n",
    "    \"\"\"\n",
    "    Compute class posterior probabilities P(c_k | v_i).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : list of np.ndarray\n",
    "        List of samples (binarized vectors), shape (num_samples, d).\n",
    "\n",
    "    class_representative_lst : list of np.ndarray\n",
    "        List of class representative vectors, one per class.\n",
    "\n",
    "    priors_lst : list or np.ndarray\n",
    "        Class priors P(c_k), should sum to 1.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    posteriors : np.ndarray\n",
    "        Array of shape (num_samples, num_classes) contining the posterior P(c_k | x_i) for each class c_k and vector v_i\n",
    "    \"\"\"\n",
    "\n",
    "    log_priors = np.log(np.array(priors_lst))\n",
    "    log_likelihoods = np.zeros((len(X), len(class_representative_lst)))\n",
    "    # Your code here \n",
    "    \n",
    "    # Compute log P(x_i | C_k) for each class\n",
    "\n",
    "    # Add log priors\n",
    "\n",
    "    # Normalize\n",
    "\n",
    "\n",
    "\n",
    "representatives_lst = [representative_X_1_vec, representative_X_3_vec, representative_X_8_vec]\n",
    "X = np.array([example_1_X_bin_vec, example_3_X_bin_vec, example_8_X_bin_vec])\n",
    "priors = [0.1, 0.3, 0.6]\n",
    "\n",
    "class_posteriors = compute_class_posteriors(X, representatives_lst, priors)\n",
    "\n",
    "expected_class_posteriors = np.array([\n",
    "    [1.00000000e+00, 1.43676230e-34, 2.25592315e-33],\n",
    "    [1.36608608e-89, 9.99947360e-01, 5.26398117e-05],\n",
    "    [8.79971746e-23, 1.49211952e-27, 1.00000000e+00],\n",
    "  ])\n",
    "\n",
    "print(f\"Expected class posteriors:\\n{expected_class_posteriors}\")\n",
    "print(f\"Class posteriors with your implementation:\\n{class_posteriors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arFMNjmb-aiu"
   },
   "source": [
    "## 3.d) Compute the conditional log-likelihood of the vectors\n",
    "\n",
    "Compute the conditional log-likelihood of the vectors by integrating over all possible class membership (note that you will have to compute an exponential of the log-probabilities computed with the previous function, again the log-sum-exp transformation might come in handy here).\n",
    "$$\n",
    "\\log P(x_1,\\ldots, x_n \\mid (c_1, \\ldots, c_K), (\\mu_1, \\ldots, \\mu_K), (\\pi_1,\\ldots, \\pi_K)) =\n",
    "\\sum_{i=1}^n \\log \\Big(\\sum_{k=1}^K \\pi_k P(x_i \\mid z_i = k)\\Big)\n",
    "$$\n",
    "`Note:` $z_i$ is the latent (non observed) variable that corresponds to the class of the vector $x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wS6l-d98-aiv"
   },
   "outputs": [],
   "source": [
    "def conditional_log_likelihood(X, class_representative_lst, priors_lst):\n",
    "    \"\"\"\n",
    "    Compute the conditional log-likelihood of each vector x_i\n",
    "    by marginalizing over all class memberships.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : list of np.ndarray\n",
    "        List of samples (binarized vectors), shape (num_samples, d).\n",
    "\n",
    "    class_representative_lst : list of np.ndarray\n",
    "        List of class representative vectors (means for each class).\n",
    "\n",
    "    priors_lst : list or np.ndarray\n",
    "        Class prior probabilities P(C_k), should sum to 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cond_log_likelihood : np.ndarray\n",
    "        1D array of conditional log-likelihoods log P(x_i) for each sample.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    log_priors = np.log(np.array(priors_lst))\n",
    "    log_likelihoods = np.zeros((len(X), len(class_representative_lst)))\n",
    "\n",
    "\n",
    "cond_log_likelihood = conditional_log_likelihood(X, representatives_lst, priors)\n",
    "\n",
    "expected_cond_log_likelihood = np.array([ -87.52055809, -178.60050244, -158.62268905])\n",
    "\n",
    "\n",
    "print(f\"Expected conditional log-likelihoods:\\n{expected_cond_log_likelihood}\")\n",
    "print(f\"Conditional log-likelihoods with your implementation:\\n{cond_log_likelihood}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3m1ojq2-aiv"
   },
   "source": [
    "## 4) Estimating class representatives and implementing EM _light_\n",
    "\n",
    "In the following we will implement a simplified version of EM with binary class allocations (each sequence is in the class where it has the highest posterior probability).\n",
    "\n",
    "We will first write a function for the E-step, then a function for the M-step, and finally combine them to EM-light.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq8X8aLe-aiv"
   },
   "source": [
    "#### 4.a) Write a function for computing the E-step of the EM algorithm\n",
    "\n",
    "From set of vectors, class representatives and priors, we compute first compute the class posteriors and then assign clusters to the input vectors. Then compute the conditional log likelihood to track convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2y6XgN9-aiv"
   },
   "outputs": [],
   "source": [
    "def E_step(X, class_representatives_lst, priors_lst):\n",
    "    \"\"\"\n",
    "    Performs the E-step of a simplified EM lite algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Data matrix of shape (n_samples, n_features), where each row is a flattened observation.\n",
    "    representatives : list of np.ndarray\n",
    "        Current class representative vectors (means or prototypes).\n",
    "    priors : np.ndarray\n",
    "        Current class prior probabilities, shape (n_classes,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    assignments : np.ndarray\n",
    "        Hard class assignments (argmax over posterior probabilities), shape (n_samples,).\n",
    "    cond_log_likelihood : np.ndarray\n",
    "        Conditional log-likelihoods log P(X), shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "    # Calculate repsonsibilities \n",
    " \n",
    "    # Hard assign\n",
    " \n",
    "    # For tracking convergence\n",
    " \n",
    " \n",
    "\n",
    "assignments, cond_log_likelihoods = E_step(X, representatives_lst, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b) Write a function for computing the M-step of the EM algorithm\n",
    "\n",
    "From set of vectors, cluster assignments from the E-step, and a given number of clusters K, update the class representatives and priors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmABVnhwJirE"
   },
   "outputs": [],
   "source": [
    "def M_step(X, assignments, K):\n",
    "    \"\"\"\n",
    "    Performs the M-step of the simplified EM algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Data matrix of shape (n_samples, n_features), where each row is a flattened observation.\n",
    "    assignments : np.ndarray\n",
    "        Hard class assignments (output of E-step), shape (n_samples,).\n",
    "    K : int\n",
    "        Number of classes (clusters).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    representatives : list of np.ndarray\n",
    "        Updated class representatives (mean vectors for each class).\n",
    "    priors : np.ndarray\n",
    "        Updated class prior probabilities (frequency of each class).\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = len(X)\n",
    "    representatives = []\n",
    "    priors = np.zeros(K)\n",
    "    # Your code here\n",
    "\n",
    "\n",
    "class_representatives_lst, priors_lst = M_step(X, assignments, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChuhCYsJ-aiv"
   },
   "source": [
    "#### 4.c) Implementing EM\n",
    "\n",
    "We will implement the EM algorithm with fixed class allocation:\n",
    "\n",
    "1. Fix the number of clusters $K$. In the next part, we first will try two values: $K=10$ and $K=20$. \n",
    "2. Initialize random allocation of the representative. The most common way is to sample the $K$ representative from the set of points at random, however you can also use e.g.: `kmeans++` for better initialization, which can lead to faster convergence (there are some libraries implementing `kmeans++` that you can use).\n",
    "3. Until convergence\n",
    "    - run E-step\n",
    "    - run M-step (update representatives and priors)\n",
    "    - check if convergence criteria is met (change in max nll or min ll is smaller than tolerance threshold `tol`) or maximum number of iterations it reached\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kV-C7xrf-aiv",
    "outputId": "c32456ca-78d5-4e2e-fee8-8656f9a9dcb6"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import kmeans_plusplus\n",
    "\n",
    "def EM(X, K, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Simplified EM (hard assignment) algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Data matrix of shape (n_samples, n_features).\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of EM iterations.\n",
    "    tol : float, optional\n",
    "        Convergence tolerance for change in log-likelihood.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    class_representatives_lst : list of np.ndarray\n",
    "        Estimated class representative vectors.\n",
    "    priors : np.ndarray\n",
    "        Estimated class priors.\n",
    "    assignments : np.ndarray\n",
    "        Hard assignments for each sample.\n",
    "    log_likelihoods : list of float\n",
    "        Log-likelihood at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Init\n",
    "    rng = np.random.default_rng()\n",
    "    init_indices = rng.choice(len(X), K, replace=False)\n",
    "\n",
    "K = 10 \n",
    "K = 20\n",
    "N = 6000\n",
    "\n",
    "class_representatives_lst, priors_lst, assignments, log_likelihoods = EM(X_train_bin[:N], K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Brk_BNcAYZVE"
   },
   "source": [
    "## 5) Evaluation\n",
    "\n",
    "Evaluating the quality of a clustering is always a complex task. In the case of the digit data, we can compare it to the digit allocation in order to assess if some clusters are better defined than others. \n",
    "\n",
    "In this part we will perform some simple evaluation and visualization for the typical clustering we obtained in the previous part (*e.g.* with $K=10$ and $K=20$ clusters). \n",
    "\n",
    "One measure that can be computed in the case where class membership are available is the **purity**. The purity is a measure of how much of a cluster describes a single class. It is defined for each cluster $C_k$ as the proportion accounted by the majority class. \n",
    "$$\n",
    "\\text{purity}(C_k) = \\frac{1}{C_k}\\; \\max_{c=0,\\ldots, 9} \\quad \\sum_{i=1}^{n} \\mathbb{I}\\{x_i \\in C_k; y_i = c\\}\n",
    "$$\n",
    "\n",
    "\n",
    "### 5.a) Plot the representative for each cluster\n",
    "- How does the representatives looks like for $K=10$ clusters? Is it a sufficient number of clusters? How can you explain that?\n",
    "- In the case of $K=20$, do you see any digits that seem to be spread across multiple cluster? How do you explain that? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3QPYBzpXSev"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for k in range(K):\n",
    "  ## Plot the representative for each cluster\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.b) Compute the purity for each cluster \n",
    "\n",
    "For both $K=10$ and $K=20$, look at the clusters that get the best or the worst purity. \n",
    "- Are they associated with particular digits? \n",
    "- How could you explain that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix\n",
    "\n",
    "def per_cluster_purity(y_true, y_pred):\n",
    "  #Your code here \n",
    "\n",
    "  return purity, cluster_sizes\n",
    "\n",
    "purity, sizes = per_cluster_purity(Y_train[:N], assignments)\n",
    "for j, (p, n) in enumerate(zip(purity, sizes)):\n",
    "    print(f\"Cluster {j}: purity={p:.3f}, size={n}\")\n",
    "\n",
    "\n",
    "ari = adjusted_rand_score(Y_train[:N], assignments)\n",
    "print(f\"ARI: {ari}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.c) Using Sankey Plot\n",
    "\n",
    "[Sankey plots](https://en.wikipedia.org/wiki/Sankey_diagram) can summarize visually how globally digit identity are spread around clusters. Create a Sankey plot that matches the digits labels to the cluster allocation. In other words we will show how the digits (labelled from the _ground truth_ `Y_train`) flow into the different clusters. You can limit yourself to the case of $K=20$ for this representation. \n",
    " \n",
    " - Consider one of the clusters with the highest purity, can you see it in the Sankey plot? \n",
    " - Is there a digit that is reliably represented by only one cluster? \n",
    " - If not how many clusters are needed at minimum to represent all instances of a digit? Which digits are concerned in that case? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotly provides the easiest method for obtaining sankey plot with interactive visualization\n",
    "import plotly.graph_objects as go \n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = 'browser'\n",
    "\n",
    "y_true = np.array(Y_train[:6000])\n",
    "cluster_assignments = np.array(assignments)\n",
    "\n",
    "true_labels = np.unique(y_true)\n",
    "cluster_labels = np.unique(cluster_assignments)\n",
    "\n",
    "source_labels = [f\"Label {t}\" for t in true_labels]\n",
    "target_labels = [f\"Cluster {c}\" for c in cluster_labels]\n",
    "all_labels = source_labels + target_labels\n",
    "\n",
    "# Match labels and create Sankey plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The agony of choice (for a good $K$)\n",
    "\n",
    "In this part we will see how we can use measures such as dispersion of likelihood to decide a good number of clusters $K$.\n",
    "\n",
    "#### 6.a) Find a good number of clusters computationally. \n",
    "\n",
    "The negative log likelihood (`nll`) and / or the within-cluster variance can be good indicators to select the number of cluster. We will consider different values of $k$ (from 2 to 20) and monitor how the `nll` or the variance changes. We will also see how the number of cluster affect the runtime of the EM-light algorithm.  Plot and describe your findings.\n",
    "\n",
    "- Run the mixture model for different values of $k$ from $2$ to $20$ or more and plot the `nll` as a function of $k$. Plot as well the running time of EM as a function of $k$. \n",
    "- Propose the best value for $k$, justify your choice. What about the resulting groups, are they very diverse?\n",
    "\n",
    "#### 6.b) Comparing the best results\n",
    "\n",
    "Using a Sankey plot with 3 components, represent the class allocation together with the clustering of the best value of $K$ and a value of $K=20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "max_nll_lst, time_lst = [], []\n",
    "\n",
    "Kmax = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Data Imputation\n",
    "### Oh no! Part of the image got overwritten with `512` values! Can we recover the digit pictured on the image with our trained mixture model?\n",
    "\n",
    "As we saw during the lecture, clustering is also a way of producing simplified representations of the data points (albeit somehow crude). We will see in the following if we can use the information in the representative to reconstruct part of the image that would have been masked. \n",
    "\n",
    "Below you will find our example image vectors, each masked with `512`s for some predetermined positions between pixels 100 and 400. This leaves us with some visible pixels $x_{vis}$ and some 'missing' (masked) pixels $x_{miss}$. \n",
    "\n",
    "We want to compute a probability for the image to below of each of the class, given the visible pixels $x_{vis}$. Then, we can reconstruct each pixel as a posterior value over all possible class allocations. \n",
    "\n",
    "1. Implement the function `em_impute`, that computes the posterior probabilities to belong to each class $c$ given the visible pixels of the image, given the results of the EM-lite run:\n",
    "\n",
    "$$\n",
    "p(z = c \\mid x_{vis})\n",
    "$$\n",
    "\n",
    "2. Then for each missing pixel $x_j$ impute its expected value given the model parameters we trained:\n",
    "$$\n",
    "\\mathbb{E}[x_j \\mid x_{vis}] = \\sum_{k=1}^{K}p(z = c_k \\mid x_{vis})\\cdot \\mu_{k,j}\n",
    "$$\n",
    "\n",
    "1. (optional) Imagine we want to consider those 3 images with missing pixel for the estimation of the parameters. Is it possible to write a version of the EM algorithm that would take into account those images with missing values? What are the changes for the E-step and the M-step in that case? \n",
    "\n",
    "\n",
    "#### 7.a) Implement `em_impute()` that takes a masked image vector, the respective mask, a set of representatives and a set of priors as input and outputs a recovered image vector with imputed values at the positions of the mask.\n",
    "\n",
    "#### 7.b) Plot the imputed images for example 1,3, and 8 with `mask1` and `mask2` respectively. Comment on the quality of the reconstruction depending on the digit and the mask used. How would you assess the quality of the reconstruction in a more systematic way? \n",
    "\n",
    "\n",
    "`Note:` You might want to make use of `log_probabilities()`, `compute_class_posteriors()`, and `binarize_image()` functions again. \n",
    "Can you get away without calling `binarize_image()`? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "example_1_masked = example_1_X_bin_vec.copy().astype('uint16')\n",
    "example_3_masked = example_3_X_bin_vec.copy().astype('uint16')\n",
    "example_8_masked = example_8_X_bin_vec.copy().astype('uint16')\n",
    "mask1 = np.ones(len(example_1_X_bin_vec), dtype = bool)\n",
    "mask2 = np.ones(len(example_1_X_bin_vec), dtype = bool)\n",
    "mask1[200:400] = False\n",
    "mask2[200:600] = False\n",
    "example_1_masked[~mask1] = 512\n",
    "example_3_masked[~mask1] = 512\n",
    "example_8_masked[~mask1] = 512\n",
    "\n",
    "cmap = ListedColormap(['black', 'white', 'red'])\n",
    "norm = BoundaryNorm([0, 0.5, 1.5, 512.5], cmap.N)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(recover_image_matrix(example_1_masked), cmap = cmap, norm=norm)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(recover_image_matrix(example_3_masked), cmap = cmap, norm=norm)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(recover_image_matrix(example_8_masked), cmap = cmap, norm=norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_impute(x_masked, mask, class_representatives, class_priors, K = 10):\n",
    "    \"\"\"\n",
    "    Imputes missing (masked) pixels in x_masked using parameters retrived by EM-light.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_masked : np.ndarray, shape (num_samples, d)\n",
    "        Each row is a flattened image (some pixels missing or zeroed)\n",
    "    mask : np.ndarray, shape (d,)\n",
    "        Boolean array: True for observed pixels, False for missing ones\n",
    "    class_representatives : np.ndarray, shape (K, d)\n",
    "        Mean vectors estimated by EM (_k)\n",
    "    class_priors : np.ndarray, shape (K,)\n",
    "        Prior probabilities for each class (_k)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_imputed : np.ndarray, shape (n_samples, d)\n",
    "        Reconstructed images with missing pixels imputed.\n",
    "    \"\"\"\n",
    "\n",
    "    class_representatives = np.array(class_representatives)\n",
    "    x_masked = np.array(x_masked).astype(float)\n",
    "        \n",
    "    # Visible pixels\n",
    "    x_vis = x_masked[mask]\n",
    "    #Your code here\n",
    "\n",
    "\n",
    "\n",
    "class_representatives_lst, priors_lst, assignments, log_likelihoods\n",
    "example_1_imputed = em_impute(example_1_masked, mask1, class_representatives_lst, priors_lst, K = 20)\n",
    "example_3_imputed = em_impute(example_3_masked, mask1, class_representatives_lst, priors_lst, K = 20)\n",
    "example_8_imputed = em_impute(example_8_masked, mask1, class_representatives_lst, priors_lst, K = 20)\n",
    "\n",
    "plt.imshow(example_8_X_bin_mat, cmap = 'grey')\n",
    "plt.imshow(recover_image_matrix(example_3_masked), cmap = 'grey')\n",
    "plt.imshow(recover_image_matrix(example_3_imputed), cmap = 'grey')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) One size _fits_ all? (Bonus)\n",
    "\n",
    "Enough with all these numbers and digits! Around 2018, Zalando research proposed a new [`fashion-MNIST`](https://github.com/zalandoresearch/fashion-mnist) dataset which consider images of fashion items. This dataset contains as well 10 classes and 60,000 examples.It can be easily imported from the `keras` package. \n",
    "\n",
    "- Load the dataset and see how our model works on this dataset too! \n",
    "  - What is the effect of the binarization of the images in part 2? \n",
    "  - You can go over parts 4.c) to 6) on this dataset and analyze the results. \n",
    "- Does the Bernoulli mixture fits to this dataset? \n",
    "- Do you have to retrain the model parameters? What do you see?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(F_train, E_train), (test_F, test_e) = fashion_mnist.load_data()\n",
    "\n",
    "# You code here\n",
    "\n",
    "example_Fashion = F_train[220]\n",
    "\n",
    "plt.imshow(example_Fashion, cmap = 'grey')\n",
    "F_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_train_bin = np.array([binarize_image(x) for x in F_train])\n",
    "\n",
    "repr, priors, assignments, lls = EM(F_train_bin[:N], 20)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs-apml-assignments-mElxrNIC-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
