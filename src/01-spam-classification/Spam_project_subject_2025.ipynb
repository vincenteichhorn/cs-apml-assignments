{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam or Ham (project 1)\n",
    "\n",
    "## 1- Introduction with the Bernoulli model\n",
    "\n",
    "\n",
    "We say that a random variables $X \\in \\{0, 1\\}$ follows a Bernoulli distribution of parameter $\\theta$ if $\\mathbb{P}(X = 1) = \\theta$ and $\\mathbb{P}(X = 0) = 1 − \\theta$.\n",
    "\n",
    "\n",
    "1.  Show that we can write the probability distribution of $X$ in a compact form as : \n",
    "$$\n",
    "\\mathbb{P}(X = x) = \\theta^{x} (1 − \\theta)^{1−x}\n",
    "$$\n",
    "\n",
    "2. Suppose now that we have a set of n independent variables $x_1,...,x_n$. If we note $n_1 = \\sum_{i=1}^n \\mathbb{1}_{\\{x_i=1\\}}$ and $n_0 = n − n_1$, show that :\n",
    "$$\n",
    "\\mathbb{P}(x_1,\\ldots,x_n \\mid \\theta)=\\theta^{n_1} (1−\\theta)^{n_0}\n",
    "$$\n",
    "\n",
    "3. Show that the maximum likelihood estimator is $\\hat{\\theta}_{ML} = \\frac{n_1}{n}$\n",
    "\n",
    "4. A conjugate prior for the Bernoulli distribution is the Beta distribution. \n",
    "$$\n",
    "Beta(\\theta \\mid a, b) \\propto \\theta^{a−1} (1 − \\theta)^{b−1}\n",
    "$$\n",
    "The Beta distribution has the following properties for its expectation and mode (for more details you can look in one of the books like the Bishop):\n",
    "$$\n",
    "\\mathbb{E}(\\theta) = \\frac{a}{a+b} \\text{, mode}(\\theta) = \\frac{a-1}{a+b-2}\n",
    "$$\n",
    "\n",
    "Show that with a $Beta(a,b)$ prior the posterior distribution \n",
    "$\\mathbb{P}(\\theta \\mid x_1,\\ldots, x_n)$\n",
    "is proportional to $\\theta^{n_1+a-1} \\cdot (1 − \\theta)^{n_0+b-1}$ \n",
    "\n",
    "5. (Those two questions are optional, you can also simply use the result in the following)\n",
    "\n",
    "  a. Show that the maximum a posteriori _mode_ estimate is in the form $\\bar{\\theta}_{MAP} = \\frac{n_1+a-1}{n+a+b-2}$\n",
    "\n",
    "  b. Show that the maximum a posteriori \n",
    "_mean_ estimate is in the form $\\hat{\\theta}_{MAP} = \\mathbb{E}(\\theta \\mid x_1,\\ldots, x_n) = \n",
    "\\int_{\\theta =0}^1 \\mathbb{P}(\\theta \\mid x_1\\ldots x_n) d\\theta  = \\frac{n_1+a}{n+a+b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "We have two cases:\n",
    "   - if $x = 1$, then $\\mathbb{P}(X = x) = \\theta = \\theta^{1} (1 − \\theta)^{0}$\n",
    "   - if $x = 0$, then $\\mathbb{P}(X = x) = 1 − \\theta = \\theta^{0} (1 − \\theta)^{1}$. \n",
    "\n",
    "Thus, the compact form holds for both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "So we have $n_1$ as the count of variables equal to 1 and $n_0$ as the count of variables equal to 0. Since the variables are independent, we can multiply their probabilities to get the joint probabilitys:\n",
    "$$\\mathbb{P}(x_1, \\dots, x_n \\mid \\theta) = \\prod_{i=1}^{n} \\mathbb{P}(X = x_i) = \\prod_{i=1}^{n_1} \\mathbb{P}(X = 1) \\prod_{j=1}^{n_0} \\mathbb{P}(X = 0)$$\n",
    "Expanding both products gives:\n",
    "$$\\prod_{i=1}^{n_1} \\mathbb{P}(X = 1) = \\prod_{i=1}^{n_1} \\theta = \\theta^{n_1}$$\n",
    "$$\\prod_{j=1}^{n_0} \\mathbb{P}(X = 0) = \\prod_{j=1}^{n_0} (1 - \\theta) = (1 - \\theta)^{n_0}$$\n",
    "Therefore, we have:\n",
    "$$\\mathbb{P}(x_1, \\dots, x_n \\mid \\theta) = \\theta^{n_1} (1 - \\theta)^{n_0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3\n",
    "To find the maximum likelihood estimator (MLE) $\\hat{\\theta}_{ML}$, we need to define the likelihood function based on the joint probability derived in Solution 2:\n",
    "$$L(\\theta) = \\mathbb{P}(x_1, \\dots, x_n \\mid \\theta) = \\theta^{n_1} (1 - \\theta)^{n_0}$$\n",
    "Now we take the natural logarithm of the likelihood function to simplify the differentiation process:\n",
    "$$\\log L(\\theta) = n_1 \\log(\\theta) + n_0 \\log(1 - \\theta)$$\n",
    "Now, we differentiate the log-likelihood with respect to $\\theta$ and set it to zero to find the maximum:\n",
    "$$\\frac{d}{d\\theta} \\log L(\\theta) = \\frac{n_1}{\\theta} - \\frac{n_0}{1 - \\theta} = 0$$\n",
    "Solving for $\\theta$, we get by cross-multiplying:\n",
    "$$n_1 (1 - \\theta) = n_0 \\theta$$\n",
    "$$n_1 - n_1 \\theta = n_0 \\theta$$\n",
    "$$n_1 = (n_0 + n_1) \\theta$$\n",
    "Thus, we find (noting that $n = n_0 + n_1$):\n",
    "$$\\hat{\\theta}_{ML} = \\frac{n_1}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4\n",
    "Using Bayes' theorem, we derive the posterior distribution as follows:\n",
    "$$\n",
    "\\mathbb{P}(\\theta \\mid x_1,\\ldots, x_n) = \\frac{\\mathbb{P}(x_1,\\ldots, x_n \\mid \\theta) \\cdot \\mathbb{P}(\\theta)}{\\mathbb{P}(x_1,\\ldots, x_n)}$$\n",
    "Therefore, we have:\n",
    "$$\\mathbb{P}(\\theta \\mid x_1,\\ldots, x_n) \\propto \\mathbb{P}(x_1,\\ldots, x_n \\mid \\theta) \\cdot \\mathbb{P}(\\theta)$$\n",
    "Now, we substitute the likelihood and prior distributions:\n",
    "$$\\mathbb{P}(\\theta \\mid x_1,\\ldots, x_n) \\propto \\theta^{n_1} (1−\\theta)^{n_0} \\cdot \\theta^{a−1} (1 − \\theta)^{b−1}$$\n",
    "Combining the exponents, we get:\n",
    "$$\\mathbb{P}(\\theta \\mid x_1,\\ldots, x_n) \\propto \\theta^{n_1+a-1} \\cdot (1 − \\theta)^{n_0+b-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5 (optional)\n",
    "Using the results from Solution 4, we can see that the posterior distribution is a Beta distribution:\n",
    "$$\\mathbb{P}(\\theta \\mid x_1,\\ldots, x_n) \\sim \\text{Beta}(n_1 + a, n_0 + b)$$\n",
    "This means that the parameters are $a' = n_1 + a$ and $b' = n_0 + b$. We can now plug them into the mode formula:\n",
    "$$\\bar{\\theta}_{MAP} = \\frac{(n_1 + a) - 1}{(n_1 + a) + (n_0 + b) - 2} = \\frac{n_1 + a - 1}{n + a + b - 2}$$\n",
    "\n",
    "And we can plug them into the mean formula:\n",
    "$$\\hat{\\theta}_{MAP} = \\mathbb{E}(\\theta \\mid x_1,\\ldots, x_n) = \\frac{n_1 + a}{n + a + b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam classifier\n",
    "\n",
    "The goal of this small project is to use a Naive bayes classifier to build a spam filter. To build our filter, we will use a dataset of 5,572 SMS messages put together by Tiago A. Almeida and José María Gómez Hidalgo. \n",
    "The dataset and the article describing the dataset are in the whiteboard directory together with this notebook. Of note, the SMS messages have already been processed for ease of use: all punctuation marks have been removed and the text has been transformed into lowercase. It is also common practice to remove any stop words such as `a`, `as`, `the` and to perform stemming (reduce words to their base form, such as stripping  the final `s*` in plural words, or the `*ing` from verbs (e.g., running becomes run)). For the sake of simplicity, we did not do that in this exercise.\n",
    "\n",
    "We will use a bag of word model:\n",
    " - We construct a corpus of the possible words $\\mathcal{D} = \\{w_1, \\ldots , w_d\\}$.\n",
    " - Each document is described by a vector of binary values $(x^{(1)}, \\ldots , x^{(d)})$ where $x^{(i)} = 1$ if $w_i$ occurs in the document and $0$ otherwise.\n",
    "\n",
    "The classification task is to predict for an SMS message if it is a _spam_ or a _ham_ (e.g. non-spam).\n",
    "\n",
    "Our data is thus $\\mathbf{x} = (x^{(1)},\\ldots, x^{(d)})$, $x^{(i)} \\in \\{0,1\\}$ and $y \\in \\{s,h\\}$\n",
    "We hypothesise that the values $x^{(i)}$ are drawn according to a Bernoulli distribution whose parameter depends on the class:\n",
    "$$\n",
    "\\mathbb{P}(x^{(i)} \\mid y = s) = \\theta_{i,s}^{x^{(i)}} \\cdot (1-\\theta_{i,s})^{1-x^{(i)}}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\mathbb{P}(x^{(i)} \\mid y = h) = \\theta_{i,h}^{x^{(i)}} \\cdot (1-\\theta_{i,h})^{1-x^{(i)}}\n",
    "$$\n",
    "As we will use a naive Bayes classifier, the occurences of the different words are independent from each other.\n",
    "\\begin{align}\n",
    "\\mathbb{P}(\\mathbf{x} \\mid y = s) & = \\prod_{i=1}^{d} \\mathbb{P} (x^{(i)} \\mid y = s)\\\\\n",
    "  & =  \\prod_{i=1}^{d} \\theta_{i,s}^{x^{(i)}} \\cdot (1-\\theta_{i,s})^{1-x^{(i)}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing library and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "##Load the dataset (if in the same directory as the notebook)\n",
    "sms_data = np.loadtxt(\"./SMSSpamCollection_cleaned.csv\", delimiter=\"\\t\", skiprows=1, dtype=str)\n",
    "\n",
    "## create test data set for checkpointing\n",
    "checkpoint_data = np.array([['spam', 'dear researcher submit manuscript money'], \n",
    "          ['ham','dear friend meet beer'],\n",
    "          ['ham', 'dear friend meet you']], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ham',\n",
       "        'go until jurong point  crazy   available only in bugis n great world la e buffet    cine there got amore wat   '],\n",
       "       ['ham', 'ok lar    joking wif u oni   '],\n",
       "       ['spam',\n",
       "        'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005  text fa to 87121 to receive entry question std txt rate t c s apply 08452810075over18 s'],\n",
       "       ...,\n",
       "       ['ham',\n",
       "        'pity    was in mood for that  so   any other suggestions '],\n",
       "       ['ham',\n",
       "        'the guy did some bitching but i acted like i d be interested in buying something else next week and he gave it to us for free'],\n",
       "       ['ham', 'rofl  its true to its name']], dtype='<U910')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Check the dataset\n",
    "sms_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n",
      "Third message is a: spam\n",
      "['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', '08452810075over18', 's']\n"
     ]
    }
   ],
   "source": [
    "##Check the size of the dataset\n",
    "num_messages = sms_data.shape[0]\n",
    "print(num_messages)\n",
    "\n",
    "##third message is a...\n",
    "print(\"Third message is a:\", sms_data[2][0])\n",
    "##dividing the third message into words\n",
    "print(sms_data[2][1].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Construction of the Corpus\n",
    "\n",
    "Construct the corpus $\\mathcal{D}$ of all words in the dataset. The corpus will be represented as a hash table where each key is a unique word in the dataset and each value is the row index for that word. \n",
    " - How many unique words are there? \n",
    " - What are the 10 most common words (_e.g._ occuring in the most documents)?\n",
    " - Transform the set of messages in the form of a binary matrix of word occurrences.\n",
    " \n",
    " You can evaluate whether your implementation works using the checkpoint_data array. For this dataset the corpus could look as follows :\n",
    "\n",
    "`{'dear': 0, 'researcher': 1, 'submit': 2, 'manuscript': 3, 'money': 4, 'friend': 5, 'meet': 6, 'beer': 7, 'you': 8}`\n",
    "(of course you could have other index values for the words). \n",
    "\n",
    "The recoding of the checkpoint data will give you the following numpy array:\n",
    "\n",
    "```\n",
    "[[1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
    " [1. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
    " [1. 0. 0. 0. 0. 1. 1. 0. 1.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_corpus(data):\n",
    "    \"\"\"\n",
    "    np.array[str, str] -> dict[str:int]\n",
    "    \n",
    "    from a 2D array of str, return a dict\n",
    "    \"\"\"\n",
    "    ##Your code here\n",
    "\n",
    "##test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def recode_messages(data, corpus):\n",
    "    \"\"\"\n",
    "    np.array[str, str] * dict[str:int] -> np.array[int, int]\n",
    "    \n",
    "    returns the binary matrix encoding \n",
    "    \"\"\"\n",
    "    ##Your code here\n",
    "\n",
    "\n",
    "D = construct_corpus(sms_data)\n",
    "\n",
    "sms_matrix = recode_messages(sms_data, D)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Construct a training and a testing set and estimation of parameters\n",
    "\n",
    "\n",
    "To do the evaluation of the model afterward we will split the dataset randomly in two: \n",
    "- one dataset for training (80% of the messages) \n",
    "- one dataset for testing (20% of the messages).\n",
    "\n",
    "If you are familiar with it, you can use the `sklearn.model_selection` functions to construct the train and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, Y, train_percentage=0.8):\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "\n",
    "    number_examples = X.shape[0]\n",
    "    num_train = int(train_percentage * number_examples)\n",
    "    ##Your code here\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Estimation of the model parameters\n",
    "\n",
    "We will now estimate our model on the training set. This means estimating two types of parameters: the class prior, and the conditional word occurrence probabilities.\n",
    "\n",
    "1.  Estimate the class prior $\\mathbb{P}(c) = \\mathbb{P}(y = c), (c = s, h)$\n",
    "2.  Using the results from section 1, compute the Maximum a posteriori estimator for the $d \\times 2$ matrix of parameters (reminder $\\theta_{i,j} = \\mathbb{P}(x=i \\mid y = j)$).\n",
    " $$\n",
    " \\Theta = \\left(\n",
    " \\begin{array}{cc}\n",
    " \\theta_{1,h}   & \\theta_{1,s} \\\\\n",
    " \\theta_{2,h} & \\theta_{2,s} \\\\\n",
    " \\vdots & \\vdots \\\\\n",
    " \\theta_{d,h} & \\theta_{d,s} \\\\\n",
    " \\end{array}\n",
    " \\right).\n",
    " $$ You can use as conjugate prior a Beta(1, 1) distribution for instance (then $\\theta_{i,c} = \\frac{n_{i,c}+1}{N+2}$ where $n_{i,c}$ is the number of documents from the class $c$ where the word $w_i$ is present and $N$ is the total number of documents of that class).\n",
    "\n",
    " When applied to the checkpoint data, your $\\Theta$ matrix should look like this:\n",
    "\n",
    " ```\n",
    " # h    s\n",
    " [[0.75 0.66666667]  #'dear'\n",
    "  [0.25 0.66666667]  #'researcher'\n",
    "  [0.25 0.66666667]  #'submit'\n",
    "  [0.25 0.66666667]  #'manuscript'\n",
    "  [0.25 0.66666667]  #'money'\n",
    "  [0.75 0.33333333]  #'friend'\n",
    "  [0.75 0.33333333]  #'meet'\n",
    "  [0.5  0.33333333]  #'beer'\n",
    "  [0.5  0.33333333]] #'you'\n",
    " ```\n",
    "\n",
    "3. Represent the fitted word occurrence class conditional probabilities $(\\theta_{i,h})_{i \\in \\mathcal{D}}$ and $(\\theta_{i,s})_{i \\in \\mathcal{D}}$. You can represent it as two barplots, like shown in the lecture at the end of the \"Naive Bayes for document\" part. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here for class priors\n",
    "    \n",
    "def estimate_proportions(data_matrix):\n",
    "    \"\"\"\n",
    "    estimate the matrix theta\n",
    "    \"\"\"\n",
    "    #your code here\n",
    "    \n",
    "##\n",
    "Dico = {'dear': 0, 'researcher':1, 'money': 2, 'friend': 3 }\n",
    "#sentence 1 dear friend money \n",
    "#sentence 2 dear researcher friend\n",
    "#sentence  friend money money\n",
    "datam = np.array([[1, 0, 1, 1],[1, 1, 0, 1],[0, 0, 1, 1]], dtype = int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Message classification\n",
    "\n",
    "\n",
    "4. Classify the messages in the test set using the Maximum a posteriori rule, and evaluate the performance of the model by computing the True Positive Rate (also called Sensitivity) and the False Positive Rate (the same as 1-Specificity). When performing the evaluation, keep in mind that the test set should not have access to the dictionary that was constructed during the training step. \n",
    "\n",
    "5. The performance of the model above was obtained by using a classification threshold of $0.5$ on the posterior probability. In other words, if $\\mathbb{P}(y = s \\mid \\mathbb{x}) \\ge 0.5$ then the message is classified as spam. Draw a ROC curve for your classifier. Note that you have to consider multiple values of the threshold to draw the ROC curve.\n",
    "\n",
    "6. Why did we use the Maximum a posteriori estimator rather than the maximum likelihood one?\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Extension of the model \n",
    "\n",
    "One extension of the model is to consider a matrix of word counts instead of simply their presence/absence. \n",
    "The model will change in various ways in this case: \n",
    "   - We will count the total number of occurence in the spam or the ham set for each word.\n",
    "   - the words are now considered to occur independently along the sentence (independent Multinoulli). Thus, for a document with k words $\\mathbf{v}=(v_1,\\ldots, v_k)$\n",
    "    \\begin{align}\n",
    "    \\mathbb{P}(\\mathbf{v} \\mid y = s) \n",
    "  & =  \\prod_{t=1}^{k} p_{v_t}\n",
    "    \\end{align}\n",
    "       where $p_v$ is the probability to observe a word $v$\n",
    "       \n",
    "Note that with this new model we compute a product over the positions in the sentence while the bernoulli model did a product over all the words in the corpus.\n",
    "\n",
    "For the end of this project, you will implement the \n",
    "\n",
    "1. Rewrite the likelihood of a sentence such that it only needs on a vector of counts of each word $\\mathbb{c} = (c^{(1)}, c^{(2)}, \\ldots, c^{(d)})$. From this likelihood formulation provide ML estimators for the parameters $p_{i}^{h}$ and $p_i^{s}$ of the model. \n",
    "2. Implement the computation of the posterior class probabilities. This question can be interpreted in different ways, depending on the modeling choice. Please explain your choices. Feel free to use additional external resources (and cite them) to guide you in this task.\n",
    "3. Compare its accuracy and ROC curve with the previous model on a test set (*e.g.* go over section 3 and 4 again for this model).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-apml-assignments-jr5HUzNP-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
